---
title: "Design Decisions & Tradeoffs"
description: "How we built our observability platform"
order: 5
---

import collectorComparisonOtel from '../../../assets/diagrams/Collector-Comparison-Otel.svg';
import collectorComparisonAlloy from '../../../assets/diagrams/Collector-Comparison-Alloy.svg';

import pyroVsParca from '../../../assets/versus/Pyro-vs-Parca.svg';
import mimirVsProm from '../../../assets/versus/Mimir-vs-Prom.svg';
import tempoVsJaeger from '../../../assets/versus/Tempo-vs-Jaeger.png';

import infrastructureChoice from '../../../assets/versus/Infrastructure-Choice.svg';

## Scope of Telemetry Data

One of the first major design decisions made was on the scope of telemetry signals to include. The current version of Vispyr captures profiles, traces, and metrics, but does not include logs.

Vispyr’s initial focus was in leveraging continuous profiling and combining that with other telemetry signals. Metrics have the clearest link with profiles, as both metrics and profiles provide insight into usage of system resources. Using profiles to perform more in-depth root cause analysis for system level usage spikes identified through metrics was a natural fit for Vispyr. Traces also benefit from having profiles available, traces allow a developer to identify latency and errors at a request and service level. Profiles can then provide more detail as to what portions of code in those requests and services are driving the issues. Incorporating logs into this flow would be a natural next step, but there were additional complexities in the process.

First, the OTel telemetry signal for logs is mature, but the SDK tooling is in various states of maturity. For the JavaScript SDK, logs are still in development. In order to work around this, we would need to deploy separate instrumentation for logs and ensure that any additional  instrumentation was compatible with our pipeline and the OTLP protocols. Second, logs are unique as a telemetry signal in that they are typically already produced by developers as part of existing processes. This means that any additional instrumentation would need to account for existing processes used to produce and store logs. This is how OTel itself approaches logs, as stated on their website:  “OpenTelemetry does not define a bespoke API or SDK to create logs. Instead, OpenTelemetry logs are the existing logs you already have from a logging framework or infrastructure component.”

While incorporating logs would be a natural complement to the other three telemetry signals included in Vispyr, the benefit did not outweigh the added complexity of account for the various forms of user log management and lack of a production ready tooling in the OTel SDK. As such, we chose to omit logs for the current version of Vispyr.

## The Observability Stack

### Agent and Gateway Collectors

Deciding which collector to use for the telemetry pipeline was a choice between using the OTel collector directly from OpenTelemetry, or Grafana Alloy, an open source vendor distribution of the OTel collector.

The initial choice was to use a vendor-agnostic solution, but OTel’s lack of profile support was a heavy trade-off. As mentioned previously, OTel lacks a production ready profiler and the profiles signal is still in progress. Without an OTLP compatible profile signal, sending profiles would require bypassing the agent-gateway telemetry pipeline. This would result in isolating profile data, increasing deployment complexity, and adding additional constraints when debugging issues between Vispyr components.

<figure style="display: flex; justify-content: center; margin: 2rem 4rem;">
  <img
    src={collectorComparisonOtel.src}
    style="width: 100%"
  />
</figure>

The alternative was to replace both the agent and gateway collector with Grafana Alloy. Alloy offers support for Pyroscope, an open source continuous profiler which we chose to use for instrumentation and data storage (see below for the decision related to Pyrocope).

<figure style="display: flex; justify-content: center; margin: 2rem 4rem;">
  <img
    src={collectorComparisonAlloy.src}
    style="width: 100%"
  />
</figure>

This allowed for a unified telemetry pipeline and simplified connection between the agent collector and the Vispyr backend. The downside of this decision is that Alloy uses a different configuration format and custom collector components. Though these are specific to Alloy, the fact that this is an open source tool based on the existing OTel collector somewhat mitigates the risk of deploying vendor specific tooling. For this reason, we chose to use Alloy as both the Agent and Gateway collectors.

### Continuous Profiler

<figure style="display: flex; justify-content: center; margin: -1rem 6rem -4rem">
  <img
    src={pyroVsParca.src}
    style="width: 100%"
  />
</figure>

For profiles, two open-source contenders existed: Parca by Polar Signals and Grafana’s own Pyroscope. Parca is an eBPF-based system profiler and was the first  tool we examined. It matched our goals for a simple to deploy instrumentation tool that required minimal customization of the user’s application. However, it being an eBPF based profiler meant it came with the inherent limitations of eBPF tooling described previously in the Overview section, specifically the limitations around newer Linux kernels and lacking production ready OTLP support. Additionally, Parca is designed as a self-contained tool, with its own data storage and visualization components. While it does support connecting the data to Grafana, it does not support sending instrumentation data through our agent and gateway collectors. This tradeoff would reintroduce the data isolation issue described in the Agent & Gateway Collector decision above.

Our second option was Grafana Pyroscope, a mature open source profiler that is capable of sending telemetry through Grafana’s OTel based collector Alloy. It also has an SDK for instrumentation. This meant we could deploy instrumentation for profiles along with all other telemetry data in a single step. However, in order to maintain a single pipeline we were required to use Grafana Alloy and remove the OTel collector.

Ultimately, the benefits of Pyroscope’s maturity, ease of instrumentation, and compatibility with our agent-gateway architecture outweighed any tradeoffs.

### Data Storage

<figure style="display: flex; justify-content: center; margin: 1rem 6rem;">
  <img
    src={mimirVsProm.src}
    style="width: 100%"
  />
</figure>

The decision for the metrics data store was focused on Prometheus and Grafana Mimir. Similar to Alloy’s relationship with the OpenTelemetry Collector, Mimir is an extension of Prometheus, offering horizontal scalability, high availability, and multi-tenancy for long-term metric storage. While beneficial for a large, microservice infrastructure, the additional complexity and overhead from Mimir is unnecessary for smaller, monolith architectures, which better represents the environment of the intended Vispyr user. Without a compelling reason to deviate from the industry standard, we chose to use Prometheus as the metrics data store.

<figure style="display: flex; justify-content: center; margin: 1rem 6rem;">
  <img
    src={tempoVsJaeger.src}
    style="width: 100%"
  />
</figure>

Lastly, the decision for traces focused on Jaeger and Grafana Tempo. While similar in identity, the approach between the data stores are vastly different. Jaeger uses external databases, such as Cassandra and Elasticsearch, to store traces. In contrast, Tempo’s only dependency is basic object storage and it comes with configuration options for major cloud provider object storage services. Choosing Tempo would simplify the deployment process, as a simple AWS S3 bucket could be quickly and easily deployed along with the rest of the AWS resources in the Vispyr backend. The simplicity of setting up the underlying storage mechanism for traces made Tempo the clear choice for data storage.

### Custom UI vs Provisioned Grafana Instance

The decision for the visualization layer came as a choice between building a custom interface or leveraging Grafana's visualization tools.
The rationale for a custom UI was that it would allow us to create a streamlined experience focused exclusively on the telemetry views most relevant to our stack and our target users. This would potentially provide a more approachable entry point for teams new to observability. However, this would come at the cost of flexibility, as users would be limited to the same predefined views. Deploying the Grafana UI as the visualization layer would provide extensive capabilities to dive deep into granular data, but this would come at the cost of limited ability to customize the various visualizations and a steeper learning curve for Vispyr’s target users.

Ultimately, benefits of providing developers with the flexibility to investigate beyond any predefined views and dashboards outweighed the alternative. With the inclusion of a preprovisioned dashboard, Vispyr is still able to provide a some level of simplified views into the telemetry data while still allowing access to the Grafana's extensive querying and drilldown features for more specific data.

## Infrastructure Choices

Vispyr’s backend is fully containerized, so our first idea was to utilize the AWS Elastic Container Store (ECS), incorporating CloudMap for internetwork routing and Elastic Block Store (EBS) for data storage. EBS would not only provide a clean pipeline for container deployment, but a service such as Fargate would facilitate horizontal scalability for each data store as needed. While ECS provided many benefits to Vispyr’s infrastructure, it had an obvious trade-off: cost. ECS is an expensive service, particularly in conjunction with Fargate, pushing the cost of service into the same domain as Grafana Cloud or AWS Managed Grafana. We also considered using ECS with EC2 instances–cheaper than Fargate–but the cost was still not low enough for small teams to really benefit from our service.

<figure style="display: flex; justify-content: center; margin: 2rem 4rem;">
  <img
    src={infrastructureChoice.src}
    style="width: 100%"
  />
</figure>

From there, we decided that a single EC2 instance with trace storage in an S3 bucket made the most sense. Traces, particularly with Tempo, have the highest storage requirements of the group, utilizing object storage for data persistence and making them a perfect contender for an S3 bucket. As for metrics and profiles, a simple local storage system works perfectly, as both have a 30-day retention period. We were able to determine that a [insert EBS volume size here] EBS volume was sufficient for holding temporary data within that period.
